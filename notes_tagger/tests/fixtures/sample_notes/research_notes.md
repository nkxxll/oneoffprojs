# ML Research Notes

Reading about transformer architectures and attention mechanisms.

## Papers Read
1. "Attention Is All You Need" - Vaswani et al.
2. "BERT: Pre-training of Deep Bidirectional Transformers"

## Key Insights
- Self-attention allows parallel processing
- Positional encodings preserve sequence order
- Pre-training on large corpus improves downstream tasks
